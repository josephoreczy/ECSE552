{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRx5m7szk7w7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip data.zip"
      ],
      "metadata": {
        "id": "oG9pP-pC8Suo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe2811f-7101-4240-f5ed-e8875e9af38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "replace data/training_labels_bin.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD2rlbpq5dHB"
      },
      "outputs": [],
      "source": [
        "# Training Data\n",
        "x_train = np.array(pd.read_csv('data/training_set.csv', header=None).values)\n",
        "y_train = np.array(pd.read_csv('data/training_labels_bin.csv', header=None).values)\n",
        "x_val = np.array(pd.read_csv('data/validation_set.csv', header=None).values)\n",
        "y_val = np.array(pd.read_csv('data/validation_labels_bin.csv', header=None).values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7_1e8N6v7qX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b1aa7f-28c2-4637-9a40-4cfd98547ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8442\n",
            "8442\n",
            "1048\n",
            "1048\n"
          ]
        }
      ],
      "source": [
        "N = len(x_train)\n",
        "M = len(x_val)\n",
        "print(N)\n",
        "print(len(y_train))\n",
        "print(M)\n",
        "print(len(y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfzaYspbv--l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560135b2-5da2-4a7c-effa-2216e5a7e676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "354\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "num_feats = x_train.shape[1]\n",
        "n_out = y_train.shape[1]\n",
        "print(num_feats)\n",
        "print(n_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqTcFXyku3wX"
      },
      "outputs": [],
      "source": [
        "# add a row of 1s, to be multiplied by the bias in the first hidden layer, easy way to not have to explicitly handle the backprop of bias as a unique case\n",
        "bias_train = np.ones((x_train.shape[0], 1))\n",
        "bias_val = np.ones((x_val.shape[0], 1))\n",
        "x_train = np.append(x_train, bias_train, axis = 1)\n",
        "x_val = np.append(x_val, bias_val, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAz9fQIuxEjB"
      },
      "outputs": [],
      "source": [
        "# sigmoid function\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "# gradient of sigmoid using a common identity\n",
        "def grad_sigmoid(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "# sum of squared error\n",
        "def squared_error(guess, correct):\n",
        "  return sum((guess-correct)**2)\n",
        "\n",
        "# gradient of the sum of squared erros\n",
        "def grad_squared_error(guess, correct):\n",
        "  grad_vec = np.zeros(len(guess))\n",
        "  for j in range(len(guess)):\n",
        "    grad_vec[j] = 2*(guess[j]-correct[j])\n",
        "  return np.reshape(grad_vec, (1, len(grad_vec)))\n",
        "\n",
        "# fully connected, 2 hidden layers, vector of size 3 output\n",
        "# we assume an input of 1x355 with the last entry a 1\n",
        "class MLP:\n",
        "  def __init__(self):\n",
        "    # initialize the hidden layers [354x354], [354x354] and [354, 3] <- W3 is the out layer\n",
        "    W1 = np.random.uniform(-1, 1, num_feats*num_feats).reshape((num_feats, num_feats))\n",
        "    W2 = np.random.uniform(-1, 1, num_feats*num_feats).reshape((num_feats, num_feats))\n",
        "    W3 = np.random.uniform(-1, 1, num_feats*3).reshape((num_feats, 3))\n",
        "\n",
        "    # put the bias into the first weight matrix so the addition is a simply result of matrix multiplication\n",
        "    # initialize a bias, so only the first hidden layer has a bias\n",
        "    bias = np.random.uniform(-1, 1, 354)\n",
        "    W1 = np.append(W1, np.array([bias]), axis = 0)\n",
        "    # put the values that needed to be saved into tables\n",
        "    self.w = [W1, W2, W3]\n",
        "    self.a = [np.zeros((1, 354)), np.zeros((1, 354)), np.zeros((1, 3))]\n",
        "    self.h = [np.zeros((1, 354)), np.zeros((1, 354)), np.zeros((1, 3))] # <- h[2] is the prediction of the model\n",
        "    # grad table simply holds the gradients of the weights\n",
        "    self.grad_table = [W1, W2, W3]\n",
        "    return\n",
        "\n",
        "  # follows the algorithm in backpropagation lecture\n",
        "  def forward_pass(self, src, sink):\n",
        "    # have to reshape the inputs into matrices so matmul works as a dot\n",
        "    src = np.reshape(src, (1, 355))\n",
        "    sink = np.reshape(sink, (1, 3))\n",
        "\n",
        "    # simply forward pass using sigmoid activations\n",
        "    self.a[0] = np.matmul(src, self.w[0])\n",
        "    self.h[0] = sigmoid(self.a[0])\n",
        "    self.a[1] = np.matmul(self.h[0], self.w[1])\n",
        "    self.h[1] = sigmoid(self.a[1])\n",
        "    self.a[2] = np.matmul(self.h[1], self.w[2])\n",
        "    self.h[2] = sigmoid(self.a[2])\n",
        "    # store the sum of squared error so as to access after pass\n",
        "    self.J = squared_error(self.h[2][0], sink[0])\n",
        "\n",
        "  # follows the algorithm in backpropagation lecture, I reversed transpositions\n",
        "  def backprop(self, src, sink, lr):\n",
        "    # have to reshape the inputs into matrices so matmul works as a dot\n",
        "    sink = np.reshape(sink, (1, 3))\n",
        "    src = np.reshape(src, (1, 355))\n",
        "\n",
        "    # set g = dJ/dy_bar\n",
        "    g = grad_squared_error(self.h[2][0], sink[0])\n",
        "    # backprop the value of g and use it to traverse the chain rule of differentiation backward, updating the weights as we pass them\n",
        "    for i in range(2, -1, -1):\n",
        "      # set g = dJ/dai as dJ/dai = (dJ/dhi)*f'(ai)\n",
        "      # (as shown in class via observance that element-wise multiplication and matrix multiplication are identical for diagonal matrices)\n",
        "      g = g*grad_sigmoid(self.a[i])\n",
        "      if i==0:\n",
        "        # h0 (h[-1]) = src (the input)\n",
        "        # have dJ/dW1 = (dJ/da0)(da0/dW1) where W1(*)h0 = a1 => da0/dW1 = h0\n",
        "        self.grad_table[i] = np.matmul(src.T, g)\n",
        "      else:\n",
        "        # have dJ/dWi = (dJ/dai)(dai/dWi) where Wi(*)hi-1 = ai => dai/dWi = hi-1\n",
        "        self.grad_table[i] = np.matmul(self.h[i-1].T, g)\n",
        "      # As Wi(*)hi-1 = ai => dai/dhi-1 = Wi, set g = dJ/dai(*)Wi = (dJ/dai)(dai/dhi-1) = dJ/dhi-1\n",
        "      g = np.matmul(g, self.w[i].T)\n",
        "      # do the gradient descent AFTER propogating g to the next level\n",
        "      # ie., W = W - lr*dJ/dW\n",
        "      self.w[i] -= lr*self.grad_table[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D883qWjslBzM"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "eta = 0.1 # intial learning rate\n",
        "stepsize = 25 # epochs before changing learning rate\n",
        "max_epoch = 55"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3ZBkceaasgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b0504f-e350-4a07-c2bb-418d80562a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Changed learning rate to lr=0.010000000000000002\n",
            "Changed learning rate to lr=0.0010000000000000002\n"
          ]
        }
      ],
      "source": [
        "# create a perceptron and lists to store the average validation and training scores per epoch\n",
        "perceptron = MLP()\n",
        "validation_scores = []\n",
        "training_scores = []\n",
        "\n",
        "for epoch in range(0, max_epoch):\n",
        "    \n",
        "    # shuffle data\n",
        "    order = np.random.permutation(N)\n",
        "    \n",
        "    sse = 0\n",
        "    for n in range(0, N):\n",
        "        idx = order[n]\n",
        "\n",
        "        # get a sample (batch size=1)\n",
        "        x_in = x_train[idx]\n",
        "        y = y_train[idx]\n",
        "\n",
        "        # forward and and backward pass through using the input\n",
        "        perceptron.forward_pass(x_in, y)\n",
        "        perceptron.backprop(x_in, y, eta)\n",
        "\n",
        "        # add the sse to our running total\n",
        "        sse += perceptron.J\n",
        "\n",
        "    # divide running total by number of inputs to get average\n",
        "    train_mse = sse/N\n",
        "    # store the average for the last epoch\n",
        "    training_scores.append(train_mse)\n",
        "    \n",
        "    # repeat for the validation set but without a backward pass\n",
        "    sse = 0\n",
        "    for m in range(0, M):\n",
        "        perceptron.forward_pass(x_val[m], y_val[m])\n",
        "        sse+=perceptron.J\n",
        "        \n",
        "    val_mse = sse/M\n",
        "    validation_scores.append(val_mse)\n",
        "\n",
        "    # after stepsize epochs decrease the learning rate by an order of magnitude\n",
        "    if epoch % stepsize == 0 and epoch != 0:\n",
        "        eta *= .1\n",
        "        print('Changed learning rate to lr=' + str(eta))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "x = np.arange(55)\n",
        "plt.title(\"MSSE Scores over Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSSE\")\n",
        "plt.plot(x, training_scores, label = 'training scores')\n",
        "plt.plot(x, validation_scores, label = 'validation scores')\n",
        "plt.legend()\n",
        "plt.savefig('q1.png')\n",
        "files.download('q1.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xj6wUMUxtipJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "mlp_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxMCVC+FTUMw9AVl81g1Lw"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}