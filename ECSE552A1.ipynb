{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECSE552A1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPEqImPkq84jRdIsl1bH3Tn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vRx5m7szk7w7"
      },
      "execution_count": 789,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data\n",
        "x_train = np.array(pd.read_csv('training_set.csv', header=None).values)\n",
        "y_train = np.array(pd.read_csv('training_labels_bin.csv', header=None).values)\n",
        "x_val = np.array(pd.read_csv('validation_set.csv', header=None).values)\n",
        "y_val = np.array(pd.read_csv('validation_labels_bin.csv', header=None).values)"
      ],
      "metadata": {
        "id": "FD2rlbpq5dHB"
      },
      "execution_count": 790,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(x_train)\n",
        "M = len(x_val)\n",
        "print(N)\n",
        "print(len(y_train))\n",
        "print(M)\n",
        "print(len(y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7_1e8N6v7qX",
        "outputId": "7ad02b08-89a9-4080-d80e-82071cacff29"
      },
      "execution_count": 791,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8442\n",
            "8442\n",
            "1048\n",
            "1048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_feats = x_train.shape[1]\n",
        "n_out = y_train.shape[1]\n",
        "print(num_feats)\n",
        "print(n_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfzaYspbv--l",
        "outputId": "239ab3ef-ca72-42b8-cb94-7875e27e0371"
      },
      "execution_count": 792,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "354\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add room for bias\n",
        "bias_train = np.ones((x_train.shape[0], 1))\n",
        "bias_val = np.ones((x_val.shape[0], 1))\n",
        "x_train = np.append(x_train, bias_train, axis = 1)\n",
        "x_val = np.append(x_val, bias_val, axis = 1)"
      ],
      "metadata": {
        "id": "hqTcFXyku3wX"
      },
      "execution_count": 793,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def grad_sigmoid(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def squared_error(guess, correct):\n",
        "  return sum((guess-correct)**2)\n",
        "\n",
        "def grad_squared_error(guess, correct):\n",
        "  grad_vec = np.zeros(len(guess))\n",
        "  for j in range(len(guess)):\n",
        "    grad_vec[j] = 2*(guess[j]-correct[j])\n",
        "  return np.reshape(grad_vec, (1, len(grad_vec)))\n",
        "\n",
        "# fully connected, 2 hidden layers, vector of size 3 output\n",
        "# we assume an input of 1x355 with the last entry a 1\n",
        "class MLP:\n",
        "  def __init__(self):\n",
        "    # initialize the hidden layers\n",
        "    W1 = np.random.uniform(-1, 1, num_feats*num_feats).reshape((num_feats, num_feats))\n",
        "    W2 = np.random.uniform(-1, 1, num_feats*num_feats).reshape((num_feats, num_feats))\n",
        "    W3 = np.random.uniform(-1, 1, num_feats*3).reshape((num_feats, 3))\n",
        "\n",
        "    # put the bias into the first weight matrix so the addition is a simply result of matrix multiplication\n",
        "    # initialize a bias\n",
        "    bias = np.random.uniform(-1, 1, 354)\n",
        "    W1 = np.append(W1, np.array([bias]), axis = 0)\n",
        "\n",
        "    self.w = [W1, W2, W3]\n",
        "    self.a = [np.zeros((1, 354)), np.zeros((1, 354)), np.zeros((1, 3))]\n",
        "    self.h = [np.zeros((1, 354)), np.zeros((1, 354)), np.zeros((1, 3))]\n",
        "\n",
        "    self.grad_table = [W1, W2, W3]\n",
        "    return\n",
        "  \n",
        "  def forward_pass(self, src, sink):\n",
        "    src = np.reshape(src, (1, 355))\n",
        "    sink = np.reshape(sink, (1, 3))\n",
        "\n",
        "    self.a[0] = np.matmul(src, self.w[0])\n",
        "    self.h[0] = sigmoid(self.a[0])\n",
        "    self.a[1] = np.matmul(self.h[0], self.w[1])\n",
        "    self.h[1] = sigmoid(self.a[1])\n",
        "    self.a[2] = np.matmul(self.h[1], self.w[2])\n",
        "    self.h[2] = sigmoid(self.a[2])\n",
        "\n",
        "    self.J = squared_error(self.h[2][0], sink[0])\n",
        "\n",
        "  def backprop(self, src, sink, lr):\n",
        "    sink = np.reshape(sink, (1, 3))\n",
        "    src = np.reshape(src, (1, 355))\n",
        "\n",
        "    g = grad_squared_error(self.h[2][0], sink[0])\n",
        "    for i in range(2, -1, -1):\n",
        "      g = g*grad_sigmoid(self.a[i])\n",
        "      if i==0:\n",
        "        self.grad_table[i] = np.matmul(src.T, g)\n",
        "      else:\n",
        "        self.grad_table[i] = np.matmul(self.h[i-1].T, g)\n",
        "      g = np.matmul(g, self.w[i].T)\n",
        "      self.w[i] -= lr*self.grad_table[i]"
      ],
      "metadata": {
        "id": "dAz9fQIuxEjB"
      },
      "execution_count": 794,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters (you may change these)\n",
        "eta = 0.1 # intial learning rate\n",
        "gamma = 0.1 # multiplier for the learning rate\n",
        "stepsize = 20 # epochs before changing learning rate\n",
        "threshold = 0.01 # stopping criterion\n",
        "test_interval = 5 # number of epoch before validating\n",
        "max_epoch = 75"
      ],
      "metadata": {
        "id": "D883qWjslBzM"
      },
      "execution_count": 797,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = MLP()\n",
        "\n",
        "for epoch in range(1, max_epoch):\n",
        "    \n",
        "    order = np.random.permutation(N) # shuffle data\n",
        "    \n",
        "    sse = 0\n",
        "    for n in range(0, N):\n",
        "        idx = order[n]\n",
        "\n",
        "        # get a sample (batch size=1)\n",
        "        x_in = x_train[idx]\n",
        "        y = y_train[idx]\n",
        "\n",
        "        perceptron.forward_pass(x_in, y)\n",
        "        perceptron.backprop(x_in, y, eta)\n",
        "\n",
        "        sse += perceptron.J\n",
        "\n",
        "    train_mse = sse/N\n",
        "    print(\"Training:\", train_mse)\n",
        "\n",
        "    if epoch % test_interval == 0 or epoch == 1: \n",
        "        # [ ] test on validation set here\n",
        "        sse = 0\n",
        "        for m in range(0, M):\n",
        "          perceptron.forward_pass(x_val[m], y_val[m])\n",
        "          sse+=perceptron.J\n",
        "        \n",
        "        val_mse = sse/M\n",
        "        print(\"Validation: \", val_mse)\n",
        "        # if termination condition is satisfied, exit\n",
        "        if val_mse < threshold:\n",
        "            break\n",
        "\n",
        "    if epoch % stepsize == 0 and epoch != 0:\n",
        "        eta = eta*gamma\n",
        "        print('Changed learning rate to lr=' + str(eta))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3ZBkceaasgE",
        "outputId": "71960c79-3695-42b8-872f-c51dc3b3c602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: 1.1586270582772364\n",
            "Validation:  1.435851716962441\n",
            "Training: 0.6467186027745676\n",
            "Training: 0.4243938414746\n",
            "Training: 0.3337393938938933\n",
            "Training: 0.26841682459424687\n",
            "Validation:  0.2582387891413158\n",
            "Training: 0.22466100361668256\n",
            "Training: 0.18607037669890622\n",
            "Training: 0.17219506437123536\n",
            "Training: 0.15644343563428595\n",
            "Training: 0.12999215947158152\n",
            "Validation:  0.158086053257577\n",
            "Training: 0.12120974418514442\n",
            "Training: 0.10950199472976313\n",
            "Training: 0.1067527023738637\n",
            "Training: 0.08630176984409145\n",
            "Training: 0.08569277251318515\n",
            "Validation:  0.17890504310675995\n",
            "Training: 0.07842624161900756\n",
            "Training: 0.06879771500205109\n",
            "Training: 0.06681009145105594\n",
            "Training: 0.06211205883779723\n",
            "Training: 0.05790321198958399\n",
            "Validation:  0.11217622282174593\n",
            "Changed learning rate to lr=0.010000000000000002\n",
            "Training: 0.024203628745038456\n",
            "Training: 0.019740226162941743\n",
            "Training: 0.018785988588258723\n",
            "Training: 0.01763048573179454\n",
            "Training: 0.01745777732997442\n",
            "Validation:  0.06917558930140202\n",
            "Training: 0.01696395177949\n",
            "Training: 0.016101800197966944\n",
            "Training: 0.016058520549764522\n",
            "Training: 0.015648361234075713\n",
            "Training: 0.015740009152395318\n",
            "Validation:  0.06766355832845572\n",
            "Training: 0.015043749337545294\n",
            "Training: 0.015037926752601741\n",
            "Training: 0.01482175241833454\n",
            "Training: 0.014703710656898631\n",
            "Training: 0.0142566142989306\n",
            "Validation:  0.06429686014578749\n",
            "Training: 0.014045454566955806\n",
            "Training: 0.013986771594850674\n",
            "Training: 0.013838067339132334\n",
            "Training: 0.01377602998177017\n",
            "Training: 0.013670108095157745\n",
            "Validation:  0.06562199216030969\n",
            "Changed learning rate to lr=0.0010000000000000002\n",
            "Training: 0.012208047965488515\n",
            "Training: 0.012129039144237762\n",
            "Training: 0.012131760232562057\n",
            "Training: 0.012059309774789875\n",
            "Training: 0.012099624831944192\n",
            "Validation:  0.0651757779691546\n",
            "Training: 0.01204141896755991\n",
            "Training: 0.012056506423710645\n",
            "Training: 0.012049617570363177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XTFFgG67CpRE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}